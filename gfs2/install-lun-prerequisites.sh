#!/bin/bash

################################################################################
# Script: install-lun-prerequisites.sh
# Descri√ß√£o: Prepara√ß√£o completa de ambiente Ubuntu 22.04 para cluster GFS2
#
# FUNCIONALIDADES PRINCIPAIS:
# - Instala pacotes essenciais para cluster GFS2 (gfs2-utils, corosync, pacemaker, etc.)
# - Configura servi√ßos de cluster (multipathd, dlm_controld, lvmlockd)
# - Configura senha do usu√°rio hacluster para autentica√ß√£o do cluster
# - Cria unit files systemd personalizados para dlm_controld e lvmlockd (Ubuntu 22.04)
# - Ajusta configura√ß√£o LVM para uso em cluster (/etc/lvm/lvm.conf)
# - Detecta automaticamente devices dispon√≠veis (multipath ou diretos)
# - Cria Volume Group e Logical Volume compartilhado com tamanho otimizado
# - Configura usu√°rio/grupo para permiss√µes adequadas
# - Valida configura√ß√£o de hostname √∫nico para cluster
#
# PR√â-REQUISITOS:
# - Ubuntu 22.04 LTS
# - Acesso sudo
# - Device de storage compartilhado dispon√≠vel (LUN iSCSI, multipath, etc.)
# - Conectividade de rede entre n√≥s do cluster
#
# USO:
# 1. Execute em AMBOS os n√≥s do cluster
# 2. Siga os prompts interativos
# 3. Use a MESMA senha do hacluster em ambos os n√≥s
# 4. Ap√≥s sucesso, execute configure-lun-multipath.sh
#
# VERS√ÉO: 2.5 - Inclui configura√ß√£o autom√°tica da senha hacluster
################################################################################

function error_exit {
    echo "Erro: $1"
    exit 1
}

function check_pkg {
    dpkg -s "$1" &>/dev/null
}

function check_service_active {
    systemctl is-active --quiet "$1"
}

function check_service_enabled {
    systemctl is-enabled --quiet "$1"
}

function force_cleanup_vg {
    local vg_name="$1"
    local device="$2"
    
    echo "=== Iniciando limpeza robusta do VG: $vg_name ==="
    
    # 1. Verificar processos usando o VG
    echo "Verificando processos que podem estar usando o VG..."
    PROCESSES=$(sudo lsof /dev/$vg_name/* 2>/dev/null | grep -v COMMAND || true)
    if [ -n "$PROCESSES" ]; then
        echo "‚ö†Ô∏è Processos detectados usando o VG:"
        echo "$PROCESSES"
        read -p "Tentar continuar mesmo assim? [s/N]: " CONTINUE_WITH_PROCESSES
        CONTINUE_WITH_PROCESSES=${CONTINUE_WITH_PROCESSES,,}
        if [[ "$CONTINUE_WITH_PROCESSES" != "s" && "$CONTINUE_WITH_PROCESSES" != "y" ]]; then
            echo "Pare os processos listados e execute o script novamente."
            return 1
        fi
    fi
    
    # 2. For√ßar desativa√ß√£o de todos os LVs
    echo "For√ßando desativa√ß√£o de Logical Volumes..."
    sudo lvchange -an $vg_name --force 2>/dev/null || true
    sleep 2
    
    # 3. Remover cada LV individualmente
    echo "Removendo Logical Volumes individualmente..."
    local lvs_list=$(sudo lvs --noheadings -o lv_name $vg_name 2>/dev/null | tr -d ' ' || true)
    for lv in $lvs_list; do
        if [ -n "$lv" ]; then
            echo "Removendo LV: $lv"
            sudo lvremove -f /dev/$vg_name/$lv 2>/dev/null || true
        fi
    done
    
    # 4. Desativar o VG
    echo "Desativando Volume Group..."
    sudo vgchange -an $vg_name 2>/dev/null || true
    sleep 2
    
    # 5. Remover VG for√ßadamente
    echo "Removendo Volume Group for√ßadamente..."
    sudo vgremove -f $vg_name 2>/dev/null || true
    sleep 2
    
    # 6. Remover Physical Volume
    echo "Removendo Physical Volume do device..."
    sudo pvremove -f "$device" 2>/dev/null || true
    sleep 2
    
    # 7. Atualizar cache do LVM
    echo "Atualizando cache do LVM..."
    sudo vgscan --cache 2>/dev/null || true
    sudo pvscan --cache 2>/dev/null || true
    
    # 8. Verificar se foi removido completamente
    if sudo vgdisplay $vg_name &>/dev/null; then
        echo "‚ùå VG '$vg_name' ainda existe ap√≥s tentativas de remo√ß√£o."
        echo "Verificando detalhes restantes..."
        sudo vgdisplay $vg_name 2>/dev/null || true
        sudo lvs $vg_name 2>/dev/null || true
        
        echo ""
        echo "=== OP√á√ïES DE LIMPEZA ADICIONAL ==="
        echo "1. Tentar remo√ß√£o mais agressiva"
        echo "2. Reiniciar sistema e tentar novamente" 
        echo "3. Usar device direto (sem LVM)"
        echo "4. Abortar script"
        read -p "Escolha uma op√ß√£o [1-4]: " CLEANUP_OPTION
        
        case $CLEANUP_OPTION in
            1)
                echo "Tentando remo√ß√£o mais agressiva..."
                # Remo√ß√£o mais agressiva
                sudo dmsetup remove_all --force 2>/dev/null || true
                sudo vgremove -f $vg_name 2>/dev/null || true
                sudo pvremove -f "$device" 2>/dev/null || true
                # Verificar novamente
                if sudo vgdisplay $vg_name &>/dev/null; then
                    echo "‚ùå Falha na remo√ß√£o agressiva. Remo√ß√£o manual necess√°ria."
                    return 1
                else
                    echo "‚úî VG removido com remo√ß√£o agressiva."
                    return 0
                fi
                ;;
            2)
                echo "üí° Recomenda√ß√£o: Reinicie o sistema com 'sudo reboot' e execute o script novamente."
                exit 1
                ;;
            3)
                echo "üí° Ser√° usado o device direto $device no pr√≥ximo script."
                echo "Execute configure-lun-multipath.sh e selecione o device $device diretamente."
                return 2  # C√≥digo especial para usar device direto
                ;;
            4)
                echo "Script abortado pelo usu√°rio."
                exit 1
                ;;
            *)
                echo "Op√ß√£o inv√°lida. Abortando."
                return 1
                ;;
        esac
    else
        echo "‚úî VG '$vg_name' removido com sucesso."
        return 0
    fi
}

echo "==== Preparando ambiente Ubuntu 22.04 para LUN GFS2 em cluster ===="

PKGS=(gfs2-utils corosync dlm-controld lvm2-lockd pcs lvm2 multipath-tools)
MISSING=()

echo "Checando pacotes necess√°rios..."
for pkg in "${PKGS[@]}"; do
    if check_pkg "$pkg"; then
        echo "‚úî Pacote $pkg instalado."
    else
        MISSING+=("$pkg")
    fi
done

if [ ${#MISSING[@]} -ne 0 ]; then
    echo "Pacotes faltantes: ${MISSING[*]}"
    echo "Instalar√° estes pacotes essenciais para cluster e multipath:"
    echo "- gfs2-utils: suporte a sistema de arquivos GFS2"
    echo "- corosync: comunica√ß√£o do cluster"
    echo "- dlm-controld: Distributed Lock Manager"
    echo "- lvm2-lockd: lock do LVM para clusters"
    echo "- pcs: gerenciador Pacemaker/Corosync"
    echo "- lvm2: volumes l√≥gicos"
    echo "- multipath-tools: multipath para SAN/iSCSI/FC"
    read -p "Deseja instalar agora? [s/N]: " ans
    ans=${ans,,}
    if [[ $ans == "s" || $ans == "y" ]]; then
        sudo apt update || error_exit "Falha no apt update"
        sudo apt install -y "${MISSING[@]}" || error_exit "Falha instalando pacotes"
    else
        error_exit "Instala√ß√£o necess√°ria negada. Abortando."
    fi
else
    echo "‚úî Todos os pacotes necess√°rios j√° est√£o instalados."
fi

# === Configura√ß√£o da senha do usu√°rio hacluster ===
echo "Configurando senha para usu√°rio hacluster (necess√°rio para autentica√ß√£o do cluster)..."

if id hacluster &>/dev/null; then
    echo "‚úî Usu√°rio hacluster encontrado"
    
    # Verificar se senha j√° foi configurada via vari√°vel de ambiente
    if [ -z "$HACLUSTER_PASSWORD" ]; then
        read -s -p "Digite a senha para o usu√°rio hacluster (mesma em todos os n√≥s): " HACLUSTER_PASSWORD
        echo
        read -s -p "Confirme a senha: " HACLUSTER_PASSWORD_CONFIRM
        echo
        
        if [ "$HACLUSTER_PASSWORD" != "$HACLUSTER_PASSWORD_CONFIRM" ]; then
            error_exit "Senhas n√£o coincidem. Execute o script novamente."
        fi
    else
        echo "Usando senha fornecida via vari√°vel de ambiente HACLUSTER_PASSWORD"
    fi
    
    # Configurar senha usando o m√©todo mais compat√≠vel
    echo "Configurando senha do usu√°rio hacluster..."
    echo "$HACLUSTER_PASSWORD" | sudo passwd --stdin hacluster 2>/dev/null || {
        # Fallback para sistemas que n√£o suportam --stdin
        echo -e "$HACLUSTER_PASSWORD\n$HACLUSTER_PASSWORD" | sudo passwd hacluster
    }
    
    if [ $? -eq 0 ]; then
        echo "‚úî Senha do usu√°rio hacluster configurada com sucesso"
    else
        error_exit "Falha ao configurar senha do usu√°rio hacluster"
    fi
else
    echo "‚ö†Ô∏è Usu√°rio hacluster n√£o encontrado. Isso √© normal se os pacotes do cluster ainda n√£o foram instalados."
    echo "O usu√°rio ser√° criado automaticamente durante a instala√ß√£o dos pacotes."
fi

# Iniciar e habilitar servi√ßo pcsd (necess√°rio para autentica√ß√£o do cluster)
echo "Iniciando e habilitando servi√ßo pcsd..."
sudo systemctl enable --now pcsd
if [ $? -eq 0 ]; then
    echo "‚úî Servi√ßo pcsd iniciado e habilitado com sucesso"
else
    echo "‚ö†Ô∏è Aviso: Problema ao iniciar pcsd, mas continuando com o script..."
fi

# Verificar servi√ßo multipathd (tem unit file systemd)
echo "Verificando servi√ßo multipathd..."
if check_service_active "multipathd" && check_service_enabled "multipathd"; then
    echo "‚úî Servi√ßo multipathd ativo e habilitado."
else
    echo "Servi√ßo multipathd n√£o est√° ativo/habilitado."
    read -p "Ativar e habilitar multipathd agora? [s/N]: " r
    r=${r,,}
    if [[ $r == "s" || $r == "y" ]]; then
        sudo systemctl enable --now multipathd || error_exit "Falha ao ativar multipathd"
        echo "‚úî Servi√ßo multipathd ativado."
    else
        error_exit "Servi√ßo multipathd obrigat√≥rio n√£o habilitado. Abortando."
    fi
fi

# Tratamento especial para dlm_controld (n√£o h√° unit systemd por padr√£o)
echo "Verificando daemon dlm_controld (Distributed Lock Manager do cluster)..."
if [ -x /usr/sbin/dlm_controld ]; then
    echo "‚úî Bin√°rio dlm_controld dispon√≠vel em /usr/sbin/dlm_controld."
    if pgrep -x dlm_controld >/dev/null; then
        echo "‚úî dlm_controld j√° est√° rodando."
    else
        echo "‚ö†Ô∏è O daemon dlm_controld N√ÉO est√° rodando."
        echo "No Ubuntu 22.04, dlm_controld n√£o possui unit file systemd padr√£o."
        echo "Criando unit file personalizado para dlm_controld..."
        
        # Criar unit file personalizado
        sudo tee /etc/systemd/system/dlm_controld.service > /dev/null << 'EOF'
[Unit]
Description=DLM Control Daemon (Cluster Locked Filesystems)
After=network.target corosync.service
Requires=corosync.service

[Service]
Type=simple
ExecStart=/usr/sbin/dlm_controld -D
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
        
        # Recarregar systemd e ativar servi√ßo
        sudo systemctl daemon-reload
        
        read -p "Deseja habilitar e iniciar o dlm_controld agora? (s/N): " RESP
        RESP=${RESP,,}
        if [[ $RESP == "s" || $RESP == "y" ]]; then
            sudo systemctl enable --now dlm_controld || error_exit "Falha ao ativar dlm_controld"
            echo "‚úî dlm_controld habilitado e iniciado via systemd."
        else
            echo "‚ö†Ô∏è AVISO: dlm_controld n√£o iniciado. GFS2/DLM n√£o funcionar√° corretamente."
        fi
    fi
else
    echo "‚ùå Bin√°rio dlm_controld n√£o encontrado! Instale o pacote dlm-controld."
    error_exit "dlm_controld ausente, n√£o √© poss√≠vel prosseguir."
fi

# Tratamento especial para lvmlockd (n√£o h√° unit systemd por padr√£o)
echo "Verificando daemon lvmlockd (lock manager do LVM para cluster)..."
if [ -x /usr/sbin/lvmlockd ]; then
    echo "‚úî Bin√°rio lvmlockd dispon√≠vel em /usr/sbin/lvmlockd."
    if pgrep -x lvmlockd >/dev/null; then
        echo "‚úî lvmlockd j√° est√° rodando."
    else
        echo "‚ö†Ô∏è O daemon lvmlockd N√ÉO est√° rodando."
        echo "No Ubuntu 22.04, lvmlockd n√£o possui unit file systemd padr√£o."
        echo "Criando unit file personalizado para lvmlockd..."
        
        # Criar unit file personalizado
        sudo tee /etc/systemd/system/lvmlockd.service > /dev/null << 'EOF'
[Unit]
Description=LVM Lock Daemon
After=network.target dlm_controld.service
Requires=dlm_controld.service

[Service]
Type=simple
ExecStart=/usr/sbin/lvmlockd -D
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
        
        # Recarregar systemd
        sudo systemctl daemon-reload
        
        read -p "Deseja habilitar e iniciar o lvmlockd agora? (s/N): " RESP
        RESP=${RESP,,}
        if [[ $RESP == "s" || $RESP == "y" ]]; then
            sudo systemctl enable --now lvmlockd || error_exit "Falha ao ativar lvmlockd"
            echo "‚úî lvmlockd habilitado e iniciado via systemd."
        else
            echo "‚ö†Ô∏è AVISO: lvmlockd n√£o iniciado. O uso de LVM compartilhado pode n√£o funcionar corretamente."
        fi
    fi
else
    echo "‚ùå Bin√°rio lvmlockd n√£o encontrado! Instale o pacote lvm2-lockd."
    error_exit "lvmlockd ausente, n√£o √© poss√≠vel prosseguir."
fi

# === Configura√ß√£o LVM para Cluster (Adaptada para Ubuntu 22.04) ===
echo "Configurando LVM adequadamente para cluster sharing (Ubuntu 22.04)..."

# Backup da configura√ß√£o atual
sudo cp /etc/lvm/lvm.conf /etc/lvm/lvm.conf.backup.$(date +%Y%m%d_%H%M%S)

# Remover configura√ß√µes conflitantes ou incompat√≠veis
sudo sed -i '/use_lvmlockd/d' /etc/lvm/lvm.conf
sudo sed -i '/locking_type/d' /etc/lvm/lvm.conf  
sudo sed -i '/shared_activation/d' /etc/lvm/lvm.conf

# Aplicar APENAS configura√ß√µes compat√≠veis com Ubuntu 22.04
sudo sed -i '/^global {/a\    use_lvmlockd = 1\n    locking_type = 1' /etc/lvm/lvm.conf

echo "‚úî Configura√ß√µes LVM para cluster aplicadas (Ubuntu 22.04):"
grep -E "(use_lvmlockd|locking_type)" /etc/lvm/lvm.conf

# Reiniciar lvmlockd para aplicar configura√ß√µes
echo "Reiniciando lvmlockd para aplicar novas configura√ß√µes..."
sudo systemctl restart lvmlockd
sleep 3

if pgrep -x lvmlockd >/dev/null; then
    echo "‚úî lvmlockd configurado e funcionando para cluster"
else
    error_exit "Falha ao configurar lvmlockd para cluster"
fi

# Checagem dos servi√ßos de cluster corosync/pacemaker
echo "Verificando servi√ßos de cluster corosync e pacemaker..."
for clustsvc in corosync pacemaker; do
    if check_service_active "$clustsvc"; then
        echo "‚úî Servi√ßo $clustsvc ativo."
    else
        echo "ALERTA: Servi√ßo $clustsvc N√ÉO est√° ativo."
        read -p "Deseja continuar mesmo assim? [s/N]: " r
        r=${r,,}
        if [[ $r != "s" && $r != "y" ]]; then
            error_exit "Servi√ßo $clustsvc deve estar ativo para cluster funcionar. Abortando."
        fi
    fi
done

# === Configura√ß√£o autom√°tica de Volume LVM Compartilhado (Melhorada) ===
echo "Verificando e configurando Volumes L√≥gicos LVM para compartilhar a LUN..."

# Verificar se j√° existe volume compartilhado
lvs_sharing=$(sudo lvs -a -o vg_name,lv_name,lv_attr --noheadings 2>/dev/null | grep "w.*a")

if [ -n "$lvs_sharing" ]; then
    echo "‚úî Volumes LVM j√° existem:"
    echo "$lvs_sharing"
else
    echo "‚ö†Ô∏è Nenhum volume l√≥gico encontrado para cluster."
    
    # Detectar devices dispon√≠veis (multipath ou direto)
    CANDIDATE_DEVICES=()
    
    # Procurar devices multipath primeiro
    if ls /dev/mapper/fc-lun-* &>/dev/null; then
        CANDIDATE_DEVICES+=($(ls /dev/mapper/fc-lun-*))
    fi
    
    # Procurar other multipath devices
    if ls /dev/mapper/[0-9a-fA-F]* &>/dev/null; then
        CANDIDATE_DEVICES+=($(ls /dev/mapper/[0-9a-fA-F]* | grep -v control))
    fi
    
    # Fallback para devices diretos (sdb, sdc, etc - excluindo sda que geralmente √© SO)
    if [ ${#CANDIDATE_DEVICES[@]} -eq 0 ]; then
        CANDIDATE_DEVICES+=($(ls /dev/sd[b-z] 2>/dev/null | head -5))
    fi
    
    if [ ${#CANDIDATE_DEVICES[@]} -eq 0 ]; then
        echo "‚ùå Nenhum device candidato encontrado para criar VG compartilhado."
        error_exit "Device para LUN compartilhada n√£o encontrado."
    fi
    
    echo "Devices candidatos detectados:"
    for i in "${!CANDIDATE_DEVICES[@]}"; do
        DEVICE=${CANDIDATE_DEVICES[$i]}
        SIZE=$(lsblk -bdno SIZE "$DEVICE" 2>/dev/null)
        if [ -n "$SIZE" ]; then
            SIZE_H=$(numfmt --to=iec $SIZE)
            echo "$((i+1)). $DEVICE - Tamanho: $SIZE_H"
        else
            echo "$((i+1)). $DEVICE - (tamanho n√£o detectado)"
        fi
    done
    
    read -p "Selecione o device para criar VG compartilhado (n√∫mero): " DEVICE_NUM
    DEVICE_INDEX=$((DEVICE_NUM-1))
    
    if [ "$DEVICE_INDEX" -lt 0 ] || [ "$DEVICE_INDEX" -ge ${#CANDIDATE_DEVICES[@]} ]; then
        error_exit "Sele√ß√£o inv√°lida."
    fi
    
    SELECTED_DEVICE=${CANDIDATE_DEVICES[$DEVICE_INDEX]}
    echo "Device selecionado: $SELECTED_DEVICE"
    
    # Obter tamanho dispon√≠vel do device
    TOTAL_SIZE_BYTES=$(lsblk -bdno SIZE "$SELECTED_DEVICE")
    if [ -z "$TOTAL_SIZE_BYTES" ]; then
        error_exit "N√£o foi poss√≠vel determinar o tamanho do device $SELECTED_DEVICE"
    fi
    
    # Calcular tamanho em GB
    TOTAL_SIZE_GB=$((TOTAL_SIZE_BYTES / 1024 / 1024 / 1024))
    
    echo "Tamanho total do device: ${TOTAL_SIZE_GB}GB"
    echo "Ser√° usado TODO o espa√ßo dispon√≠vel para m√°ximo aproveitamento."
    
    read -p "Criar VG 'vg_cluster' e LV 'lv_gfs2' usando todo o espa√ßo no device $SELECTED_DEVICE? [s/N]: " CREATE_LVM
    CREATE_LVM=${CREATE_LVM,,}
    
    if [[ "$CREATE_LVM" == "s" || "$CREATE_LVM" == "y" ]]; then
        echo "Criando Volume Group compartilhado..."
        
        # === SE√á√ÉO MELHORADA: Limpeza robusta com fun√ß√£o especializada ===
        
        # Verificar se VG 'vg_cluster' j√° existe
        if sudo vgdisplay vg_cluster &>/dev/null; then
            echo "‚ö†Ô∏è Volume Group 'vg_cluster' j√° existe no sistema."
            read -p "Deseja remover completamente e recriar? [s/N]: " RECREATE_VG
            RECREATE_VG=${RECREATE_VG,,}
            if [[ "$RECREATE_VG" == "s" || "$RECREATE_VG" == "y" ]]; then
                # Usar fun√ß√£o de limpeza robusta
                force_cleanup_vg "vg_cluster" "$SELECTED_DEVICE"
                CLEANUP_RESULT=$?
                
                if [ $CLEANUP_RESULT -eq 1 ]; then
                    error_exit "Falha na limpeza do VG. Interven√ß√£o manual necess√°ria."
                elif [ $CLEANUP_RESULT -eq 2 ]; then
                    echo "‚úî Configura√ß√£o conclu√≠da. Use o device direto no pr√≥ximo script."
                    exit 0
                fi
            else
                error_exit "N√£o √© poss√≠vel prosseguir com VG existente."
            fi
        fi
        
        # === FIM DA SE√á√ÉO MELHORADA ===
        
        # Criar VG compartilhado (sintaxe correta Ubuntu 22.04)
        echo "Criando novo Volume Group..."
        sudo vgcreate --shared vg_cluster "$SELECTED_DEVICE" || error_exit "Falha ao criar VG compartilhado"
        echo "‚úî Volume Group 'vg_cluster' criado com sucesso"
        
        # Criar LV usando TODO o espa√ßo dispon√≠vel (sintaxe correta Ubuntu 22.04)
        echo "Criando Logical Volume..."
        sudo lvcreate -n lv_gfs2 -l 100%FREE vg_cluster || error_exit "Falha ao criar LV"
        echo "‚úî Logical Volume 'lv_gfs2' criado usando todo o espa√ßo dispon√≠vel"
        
        # Ativar LV (sintaxe correta Ubuntu 22.04)
        echo "Ativando Logical Volume..."
        sudo lvchange -ay /dev/vg_cluster/lv_gfs2 || error_exit "Falha ao ativar LV"
        echo "‚úî Logical Volume ativado com sucesso"
        
        # Verificar cria√ß√£o
        echo "Verificando volumes criados:"
        sudo lvs -a -o vg_name,lv_name,lv_attr,lv_size
        
    else
        echo "Cria√ß√£o de VG/LV cancelada pelo usu√°rio."
        read -p "Deseja continuar sem volume LVM? [s/N]: " CONTINUE
        CONTINUE=${CONTINUE,,}
        if [[ "$CONTINUE" != "s" && "$CONTINUE" != "y" ]]; then
            error_exit "Volume LVM √© necess√°rio para cluster GFS2."
        fi
    fi
fi

# Verifica√ß√£o de hostname √∫nico
hostname=$(hostname)
echo "Hostname atual: $hostname"
echo "Verifique se ele √© √∫nico entre os n√≥s do cluster para evitar conflitos."
read -p "Confirma que hostname √© √∫nico? [s/N]: " r
r=${r,,}
if [[ $r != "s" && $r != "y" ]]; then
    error_exit "Hostname deve ser exclusivo nos n√≥s. Abortando."
fi

# Usu√°rio/grupo para permiss√µes se necess√°rio
if ! id "morpheus-node" &>/dev/null; then
    echo "Criando usu√°rio 'morpheus-node' e grupo 'kvm' para permiss√µes comuns ..."
    sudo groupadd -f kvm
    sudo useradd -M -g kvm morpheus-node 2>/dev/null || echo "Usu√°rio 'morpheus-node' pode j√° existir."
else
    echo "Usu√°rio 'morpheus-node' j√° existe."
fi

cat << EOF

---
[‚úî] Checagem e configura√ß√£o conclu√≠das! O sistema est√° preparado para prosseguir.

‚ö†Ô∏è UNIT FILES CRIADOS (se necess√°rio):
- /etc/systemd/system/dlm_controld.service
- /etc/systemd/system/lvmlockd.service

‚úî VOLUME LVM CONFIGURADO:
- Volume Group: vg_cluster
- Logical Volume: lv_gfs2
- Device: /dev/vg_cluster/lv_gfs2 (use este no pr√≥ximo script)
- Espa√ßo: Todo o espa√ßo dispon√≠vel do device selecionado

‚úî AUTENTICA√á√ÉO DO CLUSTER:
- Usu√°rio hacluster configurado com senha
- Servi√ßo pcsd habilitado e funcionando
- Pronto para autentica√ß√£o do cluster com 'pcs host auth'

‚ö†Ô∏è RECOMENDA√á√ïES FUTURAS:
- Execute este script no OUTRO N√ì do cluster tamb√©m (usando a MESMA senha hacluster)
- Configure e inicie corretamente o cluster Corosync e Pacemaker
- Implemente STONITH (fencing) para garantir seguran√ßa do cluster
- Ap√≥s configurar ambos os n√≥s, execute configure-lun-multipath.sh
- Use o device /dev/vg_cluster/lv_gfs2 para o sistema de arquivos GFS2

üí° PR√ìXIMO PASSO - Autentica√ß√£o do Cluster:
No n√≥ principal, execute: 
sudo pcs host auth <host1> <host2> # (onde <host1> e <host2> s√£o os nomes dos n√≥s do cluster)
(Use usu√°rio: hacluster e a senha que voc√™ configurou)

EOF

exit 0
